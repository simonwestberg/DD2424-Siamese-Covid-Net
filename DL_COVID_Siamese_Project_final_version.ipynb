{
 "metadata": {
  "colab": {
   "name": "DL_COVID_Siamese_Final_Project.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "accelerator": "GPU",
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![Title](https://s2.best-wallpaper.net/wallpaper/5120x2880/1908/Squares-geometry-colorful-light-abstract-picture_5120x2880.jpg)\n",
    "\n",
    "---\n",
    "## Project in D2424 :  SICOVID-net\n",
    "\n",
    "[**KTH-DD2424-Github**](https://gits-15.sys.kth.se/rosun/DD2424-Project)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*`Authors:`*\n",
    "\n",
    "\n",
    "---\n",
    "> `Roberto Castro Sundin  |` \n",
    "[rosun@kth.se](mailto:rosun@kth.se) \n",
    "\n",
    "> `Tony Rönnqvist         |` \n",
    "[tonyr@kth.se](mailto:tonyr@kth.se)\n",
    "\n",
    "> `Luis Alejandro Sarmiento González  |`\n",
    "[lasg@kth.se](mailto:lasg@kth.se)\n",
    "\n",
    "> `Simon Westberg |`\n",
    "[swestber@kth.se](mailto:swestber@kth.se)\n",
    "\n",
    "## 1.3 : Load libraries"
   ],
   "metadata": {
    "id": "u2YFfrcJZYgA",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We may not need all the dependencies.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# We import tensorflow, we can do 2.x for 2.0 version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.contrib.training import HParams\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "\n",
    "# Keras imports, modify or add.\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, AveragePooling2D, Input\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, Lambda, MaxPool2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "# OPENCV\n",
    "import cv2\n",
    "\n",
    "# FIle manipulating libraries.\n",
    "import pickle\n",
    "import urllib\n",
    "import zipfile \n",
    "import tarfile\n",
    "from shutil import copyfile\n",
    "#import pydicom as dicom\n",
    "\n",
    "# Math and Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "import math\n",
    "import random \n",
    "\n",
    "# Imports for plotting.\n",
    "import itertools\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn"
   ],
   "metadata": {
    "id": "n37GN9pdDt6x",
    "colab_type": "code",
    "outputId": "26a74f09-c12f-4954-d3be-e3acb506ae77",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "1.15.2-dlenv_tfe\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Using TensorFlow backend.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Found GPU at: /device:GPU:0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.0 : Data prepossessing"
   ],
   "metadata": {
    "id": "viEUsFeCM1pv",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 : Load covid data set\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "lbepvc3FDsOi",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The data is preprocessed by dividing each array with 255 and the normalizing\n",
    "# the data w.r.t. the training mean and standard deviation\n",
    "\n",
    "path_pickle = \"\"\n",
    "\n",
    "# Training data\n",
    "X_normal_tr = pickle.load(open(path_pickle + \"X_normal_tr.pickle\", \"rb\"))\n",
    "X_pneumonia_tr = pickle.load(open(path_pickle + \"X_pneumonia_tr.pickle\", \"rb\"))\n",
    "X_covid_tr = pickle.load(open(path_pickle + \"X_covid_tr.pickle\", \"rb\"))\n",
    "# Testing data\n",
    "X_normal_val = pickle.load(open(path_pickle + \"X_normal_val.pickle\", \"rb\"))\n",
    "X_pneumonia_val = pickle.load(open(path_pickle + \"X_pneumonia_val.pickle\", \"rb\"))\n",
    "X_covid_val = pickle.load(open(path_pickle + \"X_covid_val.pickle\", \"rb\"))\n",
    "# Testing data\n",
    "X_normal_ts = pickle.load(open(path_pickle + \"X_normal_ts.pickle\", \"rb\"))\n",
    "X_pneumonia_ts = pickle.load(open(path_pickle + \"X_pneumonia_ts.pickle\", \"rb\"))\n",
    "X_covid_ts = pickle.load(open(path_pickle + \"X_covid_ts.pickle\", \"rb\"))"
   ],
   "metadata": {
    "id": "gVSjG7xt5-uV",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crop and resize image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Cropping of images and remove border artifacts\n",
    "\n",
    "def undo_preprocess(img, X_mean, X_std):\n",
    "    img = np.multiply(img, X_std) + X_mean\n",
    "    img *= 255\n",
    "    return img.astype(dtype=np.int)\n",
    "\n",
    "def resize_img(img, X_mean, X_std, thickness):\n",
    "    # Undo the preprocessing\n",
    "    img = undo_preprocess(img, X_mean, X_std)\n",
    "    # Resize to smaller dimension\n",
    "    img = img[thickness:-thickness-1, thickness:-thickness-1]\n",
    "    # Resize the image back to original dimension\n",
    "    return cv2.resize(img.astype(\"float32\"), (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "thickness = 20\n",
    "X_mean = pickle.load(open(\"X_mean.pickle\", \"rb\"))\n",
    "X_std = pickle.load(open(\"X_std.pickle\", \"rb\"))\n",
    "\n",
    "\n",
    "# Resize training set\n",
    "for i in range(len(X_covid_tr)):\n",
    "    X_covid_tr[i] = resize_img(X_covid_tr[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_normal_tr)):\n",
    "    X_normal_tr[i] = resize_img(X_normal_tr[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_pneumonia_tr)):\n",
    "    X_pneumonia_tr[i] = resize_img(X_pneumonia_tr[i], X_mean, X_std, thickness)\n",
    "    \n",
    "    \n",
    "# Resize validation set\n",
    "for i in range(len(X_covid_val)):\n",
    "    X_covid_val[i] = resize_img(X_covid_val[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_normal_val)):\n",
    "    X_normal_val[i] = resize_img(X_normal_val[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_pneumonia_val)):\n",
    "    X_pneumonia_val[i] = resize_img(X_pneumonia_val[i], X_mean, X_std, thickness)\n",
    "    \n",
    "    \n",
    "# Resize testing set\n",
    "for i in range(len(X_covid_ts)):\n",
    "    X_covid_ts[i] = resize_img(X_covid_ts[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_normal_ts)):\n",
    "    X_normal_ts[i] = resize_img(X_normal_ts[i], X_mean, X_std, thickness)\n",
    "    \n",
    "for i in range(len(X_pneumonia_ts)):\n",
    "    X_pneumonia_ts[i] = resize_img(X_pneumonia_ts[i], X_mean, X_std, thickness)\n",
    "\n",
    "    \n",
    "X_tr = np.concatenate((X_normal_tr, X_pneumonia_tr, X_covid_tr), axis=0)\n",
    "X_mean = np.mean(X_tr, axis=0)\n",
    "X_std = np.std(X_tr, axis=0, ddof=1)\n",
    "\n",
    "# Normalize all data w.r.t. training mean and std\n",
    "X_normal_tr = (X_normal_tr - X_mean) / X_std\n",
    "X_pneumonia_tr = (X_pneumonia_tr - X_mean) / X_std\n",
    "X_covid_tr = (X_covid_tr - X_mean) / X_std\n",
    "\n",
    "X_normal_val = (X_normal_val - X_mean) / X_std\n",
    "X_pneumonia_val = (X_pneumonia_val - X_mean) / X_std\n",
    "X_covid_val = (X_covid_val - X_mean) / X_std\n",
    "\n",
    "X_normal_ts = (X_normal_ts - X_mean) / X_std\n",
    "X_pneumonia_ts = (X_pneumonia_ts - X_mean) / X_std\n",
    "X_covid_ts = (X_covid_ts - X_mean) / X_std\n",
    "\n",
    "path_pickle = \"Resized/\"\n",
    "\n",
    "pickle.dump(X_mean, open(path_pickle + \"X_mean.pickle\", \"wb\"))\n",
    "pickle.dump(X_std, open(path_pickle + \"X_std.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(X_covid_tr, open(path_pickle + \"X_covid_tr.pickle\", \"wb\"))\n",
    "pickle.dump(X_normal_tr, open(path_pickle + \"X_normal_tr.pickle\", \"wb\"))\n",
    "pickle.dump(X_pneumonia_tr, open(path_pickle + \"X_pneumonia_tr.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(X_covid_val, open(path_pickle + \"X_covid_val.pickle\", \"wb\"))\n",
    "pickle.dump(X_normal_val, open(path_pickle + \"X_normal_val.pickle\", \"wb\"))\n",
    "pickle.dump(X_pneumonia_val, open(path_pickle + \"X_pneumonia_val.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(X_covid_ts, open(path_pickle + \"X_covid_ts.pickle\", \"wb\"))\n",
    "pickle.dump(X_normal_ts, open(path_pickle + \"X_normal_ts.pickle\", \"wb\"))\n",
    "pickle.dump(X_pneumonia_ts, open(path_pickle + \"X_pneumonia_ts.pickle\", \"wb\"))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 : Data generator siamese"
   ],
   "metadata": {
    "id": "AIYiprq7eaHs",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X_normal, X_pneumonia, X_covid, num_channels=1, batch_size=32, samples_per_epoch=5000):\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.samples_per_epoch = samples_per_epoch\n",
    "        \n",
    "        self.X_normal = X_normal\n",
    "        self.X_pneumonia = X_pneumonia\n",
    "        self.X_covid = X_covid\n",
    "\n",
    "        self.h = self.X_normal.shape[1]\n",
    "        self.w = self.X_normal.shape[2]\n",
    "        \n",
    "        self.N_normal = self.X_normal.shape[0]\n",
    "        self.N_pneumonia = self.X_pneumonia.shape[0]\n",
    "        self.N_covid = self.X_covid.shape[0]\n",
    "\n",
    "        self.same_combinations = self.get_same_combinations(self.N_covid)\n",
    "        self.different_combinations = self.get_different_combinations(self.N_covid)\n",
    "\n",
    "        self.epoch_covid = self.X_covid.reshape(-1, self.h, self.w, num_channels)\n",
    "\n",
    "        # Extract N_covid amount of normal and pneumonia imgs randomly\n",
    "        idx_normal = np.random.choice(self.N_normal, size=self.N_covid, replace=False)\n",
    "        idx_pneumonia = np.random.choice(self.N_pneumonia, size=self.N_covid, replace=False)\n",
    "        self.epoch_normal = self.X_normal[idx_normal, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "        self.epoch_pneumonia = self.X_pneumonia[idx_pneumonia, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "\n",
    "        # Shuffle combinations\n",
    "        np.random.shuffle(self.same_combinations)\n",
    "        np.random.shuffle(self.different_combinations)\n",
    "\n",
    "    def get_same_combinations(self, num):\n",
    "        lis = []\n",
    "        for i in range(num):\n",
    "            for j in range(i+1, num):\n",
    "                lis.append([i, j])\n",
    "        return lis\n",
    "\n",
    "    def get_different_combinations(self, num):\n",
    "        lis = []\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                lis.append([i, j])\n",
    "        return lis\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of batches per epoch.\"\"\"\n",
    "        return self.samples_per_epoch // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a batch with corresponding index.\n",
    "        First batch has index 0, second has index 1, and so on.\n",
    "        \"\"\"\n",
    "        pairs_a = []\n",
    "        pairs_b = []\n",
    "        labels = []\n",
    "\n",
    "        n_same = self.batch_size // 2\n",
    "        n_different = self.batch_size // 2\n",
    "\n",
    "        # Generate same-class pairs\n",
    "        same = self.same_combinations[n_same*index:n_same*(index+1)]\n",
    "\n",
    "        for i, pair in enumerate(same):\n",
    "            if i < n_same // 3:\n",
    "                # Covid\n",
    "                pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                pairs_b.append(self.epoch_covid[pair[1]])\n",
    "            elif i < 2 * (n_same // 3):\n",
    "                # Normal\n",
    "                pairs_a.append(self.epoch_normal[pair[0]])\n",
    "                pairs_b.append(self.epoch_normal[pair[1]])\n",
    "            elif i < 3 * (n_same // 3):\n",
    "                # Pneumonia\n",
    "                pairs_a.append(self.epoch_pneumonia[pair[0]])\n",
    "                pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "            else:\n",
    "                # Choose random class for remaining pairs\n",
    "                k = np.random.randint(3)\n",
    "                if k == 0:\n",
    "                    pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                    pairs_b.append(self.epoch_covid[pair[1]])\n",
    "                elif k == 1:\n",
    "                    pairs_a.append(self.epoch_normal[pair[0]])\n",
    "                    pairs_b.append(self.epoch_normal[pair[1]])\n",
    "                else:\n",
    "                    pairs_a.append(self.epoch_pneumonia[pair[0]])\n",
    "                    pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "            labels.append(0)\n",
    "        \n",
    "        # Generate different-class pairs\n",
    "        different = self.different_combinations[n_different*index:n_different*(index+1)]\n",
    "\n",
    "        for i, pair in enumerate(different):\n",
    "            if i < n_different // 3:\n",
    "                # Covid-normal\n",
    "                pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                pairs_b.append(self.epoch_normal[pair[1]])\n",
    "            elif i < 2 * (n_different // 3):\n",
    "                # Covid-pneumonia\n",
    "                pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "            elif i < 3 * (n_different // 3):\n",
    "                # Normal-pneumonia\n",
    "                pairs_a.append(self.epoch_normal[pair[0]])\n",
    "                pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "            else:\n",
    "                k = np.random.randint(3)\n",
    "                if k == 0:\n",
    "                    # Covid-normal\n",
    "                    pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                    pairs_b.append(self.epoch_normal[pair[1]])\n",
    "                elif k == 1:\n",
    "                    # Covid-pneumonia\n",
    "                    pairs_a.append(self.epoch_covid[pair[0]])\n",
    "                    pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "                else:\n",
    "                    # Normal-pneumonia\n",
    "                    pairs_a.append(self.epoch_normal[pair[0]])\n",
    "                    pairs_b.append(self.epoch_pneumonia[pair[1]])\n",
    "            labels.append(1)\n",
    "\n",
    "        pairs_a = np.asarray(pairs_a)\n",
    "        pairs_b = np.asarray(pairs_b)\n",
    "        labels = np.asarray(labels)\n",
    "    \n",
    "        # Shuffle the data\n",
    "        permutation = np.random.permutation(len(labels))\n",
    "        pairs_a = pairs_a[permutation, :, :, :]\n",
    "        pairs_b = pairs_b[permutation, :, :, :]\n",
    "        labels = labels[permutation]\n",
    "\n",
    "        return [pairs_a, pairs_b], labels\n",
    "  \n",
    "    def on_epoch_end(self):\n",
    "        # Extract N_covid amount of normal and pneumonia imgs randomly\n",
    "        idx_normal = np.random.choice(self.N_normal, size=self.N_covid, replace=False)\n",
    "        idx_pneumonia = np.random.choice(self.N_pneumonia, size=self.N_covid, replace=False)\n",
    "        self.epoch_normal = self.X_normal[idx_normal, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "        self.epoch_pneumonia = self.X_pneumonia[idx_pneumonia, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "\n",
    "        # Shuffle combinations\n",
    "        np.random.shuffle(self.same_combinations)\n",
    "        np.random.shuffle(self.different_combinations)\n"
   ],
   "metadata": {
    "id": "I-7_EAeOI5Cs",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 : Data generator single net"
   ],
   "metadata": {
    "id": "PmiqOVIsPhds",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataGeneratorSingle(keras.utils.Sequence):\n",
    "    def __init__(self, X_normal, X_pneumonia, X_covid, num_channels=1, batch_size=32):\n",
    "        self.num_channels = num_channels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.X_normal = X_normal\n",
    "        self.X_pneumonia = X_pneumonia\n",
    "        self.X_covid = X_covid\n",
    "\n",
    "        self.h = self.X_normal.shape[1]\n",
    "        self.w = self.X_normal.shape[2]\n",
    "        \n",
    "        self.N_normal = self.X_normal.shape[0]\n",
    "        self.N_pneumonia = self.X_pneumonia.shape[0]\n",
    "        self.N_covid = self.X_covid.shape[0]\n",
    "\n",
    "        self.epoch_covid = self.X_covid.reshape(-1, self.h, self.w, num_channels)\n",
    "        np.random.shuffle(self.epoch_covid)\n",
    "\n",
    "        # Extract N_covid amount of normal and pneumonia imgs randomly\n",
    "        idx_normal = np.random.choice(self.N_normal, size=self.N_covid, replace=False)\n",
    "        idx_pneumonia = np.random.choice(self.N_pneumonia, size=self.N_covid, replace=False)\n",
    "        self.epoch_normal = self.X_normal[idx_normal, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "        self.epoch_pneumonia = self.X_pneumonia[idx_pneumonia, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of batches per epoch.\"\"\"\n",
    "        return (self.N_covid * 3) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a batch with corresponding index.\n",
    "        First batch has index 0, second has index 1, and so on.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        labels = []\n",
    "        n_class = self.batch_size // 3\n",
    "\n",
    "        idx = np.arange(n_class * index, n_class * (1 + index))\n",
    "\n",
    "        for i in idx:\n",
    "            X.append(self.epoch_covid[i])\n",
    "            X.append(self.epoch_normal[i])\n",
    "            X.append(self.epoch_pneumonia[i])\n",
    "            labels.extend([0, 1, 2])\n",
    "        \n",
    "        n_imgs = len(labels)\n",
    "\n",
    "        while n_imgs < self.batch_size:\n",
    "            k = np.random.randint(2)\n",
    "            if k == 0:\n",
    "                idx_normal = np.random.choice(self.N_normal, 1, replace=False)\n",
    "                X.append(self.X_normal[idx_normal].reshape(self.h, self.w, self.num_channels))\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                idx_pneumonia = np.random.choice(self.N_pneumonia, 1, replace=False)\n",
    "                X.append(self.X_pneumonia[idx_pneumonia].reshape(self.h, self.w, self.num_channels))\n",
    "                labels.append(2)\n",
    "            n_imgs += 1\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        permutation = np.random.permutation(len(labels))\n",
    "        X = X[permutation, :, :, :]\n",
    "        labels = labels[permutation]\n",
    "        return X, labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Extract N_covid amount of normal and pneumonia imgs randomly\n",
    "        idx_normal = np.random.choice(self.N_normal, size=self.N_covid, replace=False)\n",
    "        idx_pneumonia = np.random.choice(self.N_pneumonia, size=self.N_covid, replace=False)\n",
    "        self.epoch_normal = self.X_normal[idx_normal, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "        self.epoch_pneumonia = self.X_pneumonia[idx_pneumonia, :, :].reshape(-1, self.h, self.w, self.num_channels)\n",
    "        # Shuffle covid images\n",
    "        np.random.shuffle(self.epoch_covid)\n",
    "        "
   ],
   "metadata": {
    "id": "4zafIE9j6IpO",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 : Generate validation pairs for siamese net"
   ],
   "metadata": {
    "id": "pNZvo_ZjtaE1",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_validation_pairs(trials, X_covid_val, X_normal_val, X_pneumonia_val):\n",
    "    \"\"\" \n",
    "    Returns trials amount of validation triplet-pairs for 3-way one shot learning.\n",
    "    If trials=3, the function returns \n",
    "    X_val = [   [[cov_img, cov_img], [cov_img, norm_img], [cov_img, pnem_img]], \n",
    "                [[norm_img, cov_img], [norm_img, norm_img], [norm_img, pnem_img]], \n",
    "                [[pnem_img, cov_img], [pnem_img, norm_img], [pnem_img, pnem_img]]] \n",
    "    Y_val = [   [0, 1, 1], \n",
    "                [1, 0, 1], \n",
    "                [1, 1, 0]]\n",
    "    i.e., we get three trials of 3-way one shot learning validation tasks.  \n",
    "    \"\"\"\n",
    "\n",
    "    N_covid_val, h, w = X_covid_val.shape\n",
    "    N_normal_val = X_normal_val.shape[0]\n",
    "    N_pneumonia_val = X_pneumonia_val.shape[0]\n",
    "\n",
    "    if trials % 3 != 0:\n",
    "        raise ValueError(\"Trials not a multiple of 3.\")\n",
    "\n",
    "    if trials // 3 >= N_covid_val:\n",
    "        raise ValueError(\"Not enough covid images.\")\n",
    "\n",
    "    indices_covid = np.arange(N_covid_val)\n",
    "    indices_normal = np.arange(N_normal_val)\n",
    "    indices_pneumonia = np.arange(N_pneumonia_val)\n",
    "\n",
    "    X_val = []\n",
    "    Y_val = []\n",
    "\n",
    "    for i in range(trials // 3):\n",
    "        # COVID\n",
    "        # First extract a covid-image to predict the class of\n",
    "        index = np.random.choice(indices_covid)\n",
    "        covid_img = X_covid_val[index, :, :].reshape(h, w, 1)\n",
    "        indices_covid = indices_covid[indices_covid != index]\n",
    "\n",
    "        # Extract a covid, normal, and pneumonia image to compare against in 3-way learning\n",
    "        index_covid = np.random.choice(indices_covid)\n",
    "        img1 = X_covid_val[index_covid, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_normal = np.random.choice(indices_normal)\n",
    "        img2 = X_normal_val[index_normal, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_pneumonia = np.random.choice(indices_pneumonia)\n",
    "        img3 = X_pneumonia_val[index_pneumonia, :, :].reshape(h, w, 1)\n",
    "        \n",
    "        # Append the triplet of image-pairs to X_val, and corresponding label to Y_val\n",
    "        X_val.append([[covid_img, img1], [covid_img, img2], [covid_img, img3]]) \n",
    "        Y_val.append([0, 1, 1])\n",
    "        \n",
    "        # NORMAL\n",
    "        # First extract a normal-image to predict the class of\n",
    "        index = np.random.choice(indices_normal)\n",
    "        normal_img = X_normal_val[index, :, :].reshape(h, w, 1)\n",
    "        indices_normal = indices_normal[indices_normal != index]\n",
    "\n",
    "        # Extract a covid, normal, and pneumonia image to compare against in 3-way learning\n",
    "        index_covid = np.random.choice(indices_covid)\n",
    "        img1 = X_covid_val[index_covid, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_normal = np.random.choice(indices_normal)\n",
    "        img2 = X_normal_val[index_normal, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_pneumonia = np.random.choice(indices_pneumonia)\n",
    "        img3 = X_pneumonia_val[index_pneumonia, :, :].reshape(h, w, 1)\n",
    "\n",
    "        X_val.append([[normal_img, img1], [normal_img, img2], [normal_img, img3]])\n",
    "        Y_val.append([1, 0, 1])\n",
    "        \n",
    "        # PNEUMONIA\n",
    "        # First extract a pneumonia-image to predict the class of\n",
    "        index = np.random.choice(indices_pneumonia)\n",
    "        pneumonia_img = X_pneumonia_val[index, :, :].reshape(h, w, 1)\n",
    "        indices_pneumonia = indices_pneumonia[indices_pneumonia != index]\n",
    "\n",
    "        # Extract a covid, normal, and pneumonia image to compare against in 3-way learning\n",
    "        index_covid = np.random.choice(indices_covid)\n",
    "        img1 = X_covid_val[index_covid, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_normal = np.random.choice(indices_normal)\n",
    "        img2 = X_normal_val[index_normal, :, :].reshape(h, w, 1)\n",
    "\n",
    "        index_pneumonia = np.random.choice(indices_pneumonia)\n",
    "        img3 = X_pneumonia_val[index_pneumonia, :, :].reshape(h, w, 1)\n",
    "\n",
    "        X_val.append([[pneumonia_img, img1], [pneumonia_img, img2], [pneumonia_img, img3]])\n",
    "        Y_val.append([1, 1, 0])\n",
    "    \n",
    "    return X_val, Y_val\n",
    "\n",
    "X_val, Y_val = get_validation_pairs(129, X_covid_val, X_normal_val, X_pneumonia_val)\n",
    "path_pickle = \"/content/drive/My Drive/Covid-19_datasets/data/Pickles2/\"\n",
    "#pickle.dump(X_val, open(path_pickle + \"X_val.pickle\", \"wb\"), protocol=4)\n",
    "#pickle.dump(Y_val, open(path_pickle + \"Y_val.pickle\", \"wb\"), protocol=4)"
   ],
   "metadata": {
    "id": "7Cfo5oLPtez9",
    "colab_type": "code",
    "colab": {}
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.0 :  Network architecture\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "0jIbrWGsNuGf",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 : Network"
   ],
   "metadata": {
    "id": "JokbUiCzV0Wf",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def PEPX(x, nf1, nf2, nf3, nf4, name):\n",
    "    # The PEPX-module is described as\n",
    "    # conv1x1 -> conv1x1 -> DWConv3x3 -> conv1x1 -> conv1x1\n",
    "\n",
    "    # Project input features to a lower dimension\n",
    "    # Expand features to a higher dimension that is different than that of the input features \n",
    "    # Project features back to a lower dimension, and \n",
    "    # Extend channel dimensionality to a higher dimension to produce the ﬁnal features.\n",
    "\n",
    "    x = Conv2D(filters=nf1, kernel_size=1, strides=1, activation=\"relu\", name=name + \"_Project1\")(x)\n",
    "    x = Conv2D(filters=nf2, kernel_size=1, strides=1, activation=\"relu\", name=name + \"_Expand\")(x)\n",
    "    x = DepthwiseConv2D(kernel_size=3, strides=1, activation=\"relu\", padding=\"same\", name=name + \"_DW\")(x)\n",
    "    x = Conv2D(filters=nf3, kernel_size=1, strides=1, activation=\"relu\", name=name + \"_Project2\")(x)\n",
    "    x = Conv2D(filters=nf4, kernel_size=1, strides=1, activation=\"relu\", name=name + \"_Extend\")(x)\n",
    "    return x\n",
    "\n",
    "def covid_net(input_shape, nf, fc_units, single=False):\n",
    "    \"\"\" \n",
    "    Replication of the COVID-Net architecture. \n",
    "    UL = upper layer in covid-net.\n",
    "    nf: number of filters in the first PEPX module\n",
    "    fc_units: number of units in the first fully connected layer\n",
    "    \"\"\"\n",
    "    \n",
    "    img = Input(shape=input_shape)\n",
    "\n",
    "    pep_nf = nf\n",
    "\n",
    "    # Input: 224x224x1, output: 112x112x64\n",
    "    x = Conv2D(filters=64, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\", name=\"CV_7x7\")(img)\n",
    "\n",
    "    # UL 1. Input: 112x112x64, output: 56x56xnf\n",
    "    y1 = Conv2D(filters=pep_nf, kernel_size=1, activation=\"relu\", name=\"CV_UL1\")(x)\n",
    "    y1 = MaxPool2D(2, name=\"MP_UL1\")(y1)\n",
    "\n",
    "    # PEPX1.1 - PEPX1.3. \n",
    "    # 1.1       Input: 112x112x64, output: 56x56xnf\n",
    "    # 1.2-1.3.  Input: 56x56xnf, output: 56x56xnf\n",
    "    pepx1_1 = PEPX(x, 32, 48, 32, pep_nf, \"P11\")\n",
    "    pepx1_1 = MaxPool2D(2, name=\"MP_P11\")(pepx1_1)\n",
    "    \n",
    "    pepx1_2 = keras.layers.add([pepx1_1, y1])\n",
    "    pepx1_2 = PEPX(pepx1_2, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P12\")\n",
    "\n",
    "    pepx1_3 = keras.layers.add([pepx1_1, pepx1_2, y1])\n",
    "    pepx1_3 = PEPX(pepx1_3, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf,  \"P13\")\n",
    "\n",
    "    # UL 2. Input: 56x56xnf, output: 28x28x2nf\n",
    "    y2 = keras.layers.add([pepx1_1, pepx1_2, pepx1_3, y1])\n",
    "    y2 = Conv2D(filters=2 * pep_nf, kernel_size=1, activation=\"relu\", name=\"CV_UL2\")(y2)\n",
    "    y2 = MaxPool2D(2, name=\"MP_UL2\")(y2)\n",
    "\n",
    "    # PEPX 2.1-2.4 \n",
    "    # 2.1       Input: 56x56xnf, output: 28x28x2nf\n",
    "    # 2.2-2.4   Input: 28x28x2nf, output: 28x28x2nf\n",
    "    pepx2_1 = keras.layers.add([pepx1_1, pepx1_2, pepx1_3, y1])\n",
    "    pepx2_1 = PEPX(pepx2_1, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, 2 * pep_nf, \"P21\")\n",
    "    pepx2_1 = MaxPool2D(2, name=\"MP_P21\")(pepx2_1)\n",
    "\n",
    "    pep_nf *= 2\n",
    "\n",
    "    pepx2_2 = keras.layers.add([pepx2_1, y2])\n",
    "    pepx2_2 = PEPX(pepx2_2, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P22\")\n",
    "\n",
    "    pepx2_3 = keras.layers.add([pepx2_1, pepx2_2, y2])\n",
    "    pepx2_3 = PEPX(pepx2_3, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P23\")\n",
    "\n",
    "    pepx2_4 = keras.layers.add([pepx2_1, pepx2_2, pepx2_3, y2])\n",
    "    pepx2_4 = PEPX(pepx2_4, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P24\")\n",
    "\n",
    "    # UL 3. Input: 28x28x2nf, output: 14x14x4nf\n",
    "    y3 = keras.layers.add([pepx2_1, pepx2_2, pepx2_3, pepx2_4, y2])\n",
    "    y3 = Conv2D(filters=2 * pep_nf, kernel_size=1, activation=\"relu\", name=\"CV_UL3\")(y3)\n",
    "    y3 = MaxPool2D(2, name=\"MP_UL3\")(y3)\n",
    "\n",
    "    # PEPX 3.1-3.6. \n",
    "    # 3.1       Input: 28x28x2nf, output: 14x14x4nf\n",
    "    # 3.2-3.6   Input: 14x14x4nf, output: 14x14x4nf\n",
    "    pepx3_1 = keras.layers.add([pepx2_1, pepx2_2, pepx2_3, pepx2_4, y2])\n",
    "    pepx3_1 = PEPX(pepx3_1, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, 2 * pep_nf, \"P31\")\n",
    "    pepx3_1 = MaxPool2D(2, name=\"MP_P31\")(pepx3_1)\n",
    "\n",
    "    pep_nf *= 2\n",
    "\n",
    "    pepx3_2 = keras.layers.add([pepx3_1, y3])\n",
    "    pepx3_2 = PEPX(pepx3_2, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P32\")\n",
    "\n",
    "    pepx3_3 = keras.layers.add([pepx3_1, pepx3_2, y3])\n",
    "    pepx3_3 = PEPX(pepx3_3, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P33\")\n",
    "\n",
    "    pepx3_4 = keras.layers.add([pepx3_1, pepx3_2, pepx3_3, y3])\n",
    "    pepx3_4 = PEPX(pepx3_4, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P34\")\n",
    "\n",
    "    pepx3_5 = keras.layers.add([pepx3_1, pepx3_2, pepx3_3, pepx3_4, y3])\n",
    "    pepx3_5 = PEPX(pepx3_5, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P35\")\n",
    "\n",
    "    pepx3_6 = keras.layers.add([pepx3_1, pepx3_2, pepx3_3, pepx3_4, pepx3_5, y3])\n",
    "    pepx3_6 = PEPX(pepx3_6, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P36\")\n",
    "\n",
    "    # UL 4. Input: 14x14x4nf, output: 7x7x8nf\n",
    "    y4 = keras.layers.add([pepx3_1, pepx3_2, pepx3_3, pepx3_4, pepx3_5, pepx3_6, y3])\n",
    "    y4 = Conv2D(filters=2 * pep_nf, kernel_size=1, activation=\"relu\", name=\"CV_UL4\")(y4)\n",
    "    y4 = MaxPool2D(2, name=\"MP_UL4\")(y4)\n",
    "\n",
    "    # PEPX 4.1-4.3\n",
    "    # 4.1       Input: 14x14x4nf, output: 7x7x8nf\n",
    "    # 4.2-4.3   Input: 7x7x8nf, output: 7x7x8nf\n",
    "    pepx4_1 = keras.layers.add([pepx3_1, pepx3_2, pepx3_3, pepx3_4, pepx3_5, pepx3_6, y3])\n",
    "    pepx4_1 = PEPX(pepx4_1, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, 2 * pep_nf, \"P41\")\n",
    "    pepx4_1 = MaxPool2D(2, name=\"MP_P41\")(pepx4_1)\n",
    "\n",
    "    pep_nf *= 2\n",
    "\n",
    "    pepx4_2 = keras.layers.add([pepx4_1, y4])\n",
    "    pepx4_2 = PEPX(pepx4_2, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P42\")\n",
    "\n",
    "    pepx4_3 = keras.layers.add([pepx4_1, pepx4_2, y4])\n",
    "    pepx4_3 = PEPX(pepx4_3, pep_nf // 2, 3 * pep_nf // 4, pep_nf // 2, pep_nf, \"P43\")\n",
    "\n",
    "    # Flatten and two fully connected layers, possibly dropout\n",
    "    z = keras.layers.add([pepx4_1, pepx4_2, pepx4_3, y4], name=\"OUTPUTX\")\n",
    "    \n",
    "    flat = Flatten(name=\"Flat\")(z)   # 100352x1\n",
    "    flat = keras.layers.Dropout(rate=0.50)(flat)\n",
    "    \n",
    "    fc1 = Dense(units=fc_units, activation=\"relu\", name=\"FC1\")(flat)\n",
    "    fc1 = keras.layers.Dropout(rate=0.20)(fc1)\n",
    "    \n",
    "    fc2 = Dense(units=256, activation=\"relu\", name=\"FC2\")(fc1)\n",
    "    fc2 = keras.layers.Dropout(rate=0.20)(fc2)\n",
    "    \n",
    "    if single:\n",
    "        output = Dense(units=3, name=\"Output_before_SM\")(fc2)\n",
    "        output = Activation(\"softmax\", name=\"Output_after_SM\")(output)\n",
    "        return Model(img, output)\n",
    "        \n",
    "\n",
    "    return Model(img, fc2)\n",
    "\n",
    "def siamese_net(nf, fc_units):\n",
    "    input_shape = (224, 224, 1)\n",
    "    \n",
    "    covid_model = covid_net(input_shape, nf, fc_units)\n",
    "    \n",
    "    img1 = Input(shape=input_shape, name=\"Img1\")  # First image\n",
    "    img2 = Input(shape=input_shape, name=\"Img2\")  # Second image\n",
    "    \n",
    "    feature_vec1 = covid_model(img1)\n",
    "    feature_vec2 = covid_model(img2)\n",
    "\n",
    "    l1 = Lambda(lambda tensors : keras.backend.abs(tensors[0] - tensors[1]), name=\"Lambda\")([feature_vec1, feature_vec2])\n",
    "    similarity = Dense(units=1, activation=\"sigmoid\", name= \"Similarity\")(l1)\n",
    "    \n",
    "    model = Model(inputs=[img1, img2], outputs=similarity) \n",
    "    return model\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 : 3-way validation callback"
   ],
   "metadata": {
    "id": "FtY_X4stTs-P",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Validation(keras.callbacks.Callback):\n",
    "    \"\"\"A custom validation callback for 3-way one-shot validation used in the \n",
    "    training of a siamese network.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data, tr_batches_per_epoch):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.X_val = self.validation_data[0]\n",
    "        self.Y_val = self.validation_data[1]\n",
    "        self.trials = len(self.Y_val)\n",
    "        self.val_accuracy = []\n",
    "        self.tr_batches_per_epoch = tr_batches_per_epoch\n",
    "        \n",
    "        self.max_val_acc = 0\n",
    "\n",
    "        self.confusion_matrices = []\n",
    "        self.confusion_matrices_perc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.validation()\n",
    "        accuracy = self.val_accuracy[-1]\n",
    "        print(\"\\n3-way validation accuracy {} trials - epoch: {} - accuracy: {}\".format(self.trials, epoch, accuracy))\n",
    "        print(self.confusion_matrices_perc[-1])\n",
    "        \n",
    "        # Save model if accuracy is larger than previous max validation accuracy\n",
    "        #if accuracy > self.max_val_acc:\n",
    "        #    self.max_val_acc = accuracy\n",
    "        #    self.model.save(\"Models/siameseNet\")\n",
    "\n",
    "    def validation(self):\n",
    "        correct = 0\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        for i, triplet in enumerate(self.X_val):\n",
    "          \n",
    "            pair1 = triplet[0]\n",
    "            pair2 = triplet[1]\n",
    "            pair3 = triplet[2]\n",
    "\n",
    "            sim1 = self.model.predict(x=[[pair1[0]], [pair1[1]]])\n",
    "            sim2 = self.model.predict(x=[[pair2[0]], [pair2[1]]])\n",
    "            sim3 = self.model.predict(x=[[pair3[0]], [pair3[1]]])\n",
    "            \n",
    "            predict_triplet = [sim1, sim2, sim3]\n",
    "            Y_triplet = self.Y_val[i]\n",
    "            \n",
    "            prediction = np.argmin(predict_triplet)\n",
    "            truth = np.argmin(Y_triplet)\n",
    "            \n",
    "            true_labels.append(truth)\n",
    "            predicted_labels.append(prediction)\n",
    "\n",
    "            if prediction == truth:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / self.trials\n",
    "        conf = confusion_matrix(true_labels, predicted_labels)\n",
    "        self.val_accuracy.append(np.round(accuracy, 3))\n",
    "        self.confusion_matrices.append(conf)\n",
    "        self.confusion_matrices_perc.append(np.round(conf / np.sum(conf, axis=1), 2))\n"
   ],
   "metadata": {
    "id": "Mj3eWhqGT19J",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.0 : Training and validation"
   ],
   "metadata": {
    "id": "tZi7JtgyP6Cr",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 : Hyperparameters"
   ],
   "metadata": {
    "id": "eT2y8OnXVzuf",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set hyperparameters before training\n",
    "hp = HParams(\n",
    "    # Network\n",
    "    nf=128,\n",
    "    fc_units=1024,\n",
    "    # Training\n",
    "    learning_rate=1e-4,\n",
    "    batch_size_tr=16,\n",
    "    batch_size_val=16,\n",
    "    samples_per_epoch_tr=12000,\n",
    "    samples_per_epoch_val=1000,\n",
    "    n_epochs=100,\n",
    "    workers=1,\n",
    "    # Directories\n",
    "    path_pickle=\"Resized/\",\n",
    "    path_figs=\"Figures/\",\n",
    "    checkpoint_dir='/tmp/checkpoints/ '\n",
    ")"
   ],
   "metadata": {
    "id": "7wNwcskJKehV",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 : Load training and validation data"
   ],
   "metadata": {
    "id": "XU0yZBNLS2yU",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load training and validation data to feed to data generator\n",
    "X_normal_tr = pickle.load(open(hp.path_pickle + \"X_normal_tr.pickle\", \"rb\"))\n",
    "X_pneumonia_tr = pickle.load(open(hp.path_pickle + \"X_pneumonia_tr.pickle\", \"rb\"))\n",
    "X_covid_tr = pickle.load(open(hp.path_pickle + \"X_covid_tr.pickle\", \"rb\"))\n",
    "\n",
    "X_normal_val = pickle.load(open(hp.path_pickle + \"X_normal_val.pickle\", \"rb\"))\n",
    "X_pneumonia_val = pickle.load(open(hp.path_pickle + \"X_pneumonia_val.pickle\", \"rb\"))\n",
    "X_covid_val = pickle.load(open(hp.path_pickle + \"X_covid_val.pickle\", \"rb\"))\n",
    "\n",
    "# Validation for single COVID-net\n",
    "X_val_single = pickle.load(open(hp.path_pickle + \"X_val_single.pickle\", \"rb\"))\n",
    "Y_val_single = pickle.load(open(hp.path_pickle + \"Y_val_single.pickle\", \"rb\"))\n",
    "\n",
    "# Load validation data to use in 3-way validation callback\n",
    "X_val = pickle.load(open(hp.path_pickle + \"X_val.pickle\", \"rb\"))\n",
    "Y_val = pickle.load(open(hp.path_pickle + \"Y_val.pickle\", \"rb\"))"
   ],
   "metadata": {
    "id": "uG7ehmRBScdz",
    "colab_type": "code",
    "colab": {},
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 : Train siamese model"
   ],
   "metadata": {
    "id": "K-8duKrWOMEi",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SIAMESE NET\n",
    "\n",
    "\n",
    "learning_rates = [1e-4]\n",
    "batch_sizes = [32]\n",
    "\n",
    "for bz in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(\"bz:\", bz, \"lr:\", lr)\n",
    "        print()\n",
    "\n",
    "        model = siamese_net(nf=hp.nf, fc_units=hp.fc_units)\n",
    "        adam = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "        training_generator = DataGenerator(X_normal_tr, X_pneumonia_tr, X_covid_tr, num_channels=1, \n",
    "                                           batch_size=bz, samples_per_epoch=hp.samples_per_epoch_tr)\n",
    "\n",
    "        val_callback = Validation(validation_data=(X_val, Y_val), tr_batches_per_epoch=training_generator.__len__())\n",
    "\n",
    "        history = model.fit_generator(generator=training_generator, \n",
    "                                      epochs=hp.n_epochs, callbacks=[val_callback],\n",
    "                                      use_multiprocessing=False, workers=hp.workers)\n",
    "        d = history.history\n",
    "        d_3w = {\"val_acc_3w\": val_callback.val_accuracy, \"conf\": val_callback.confusion_matrices_perc}\n",
    "        d_combined = {**d, **d_3w}\n",
    "\n",
    "        pickle.dump(d_combined, open(hp.path_pickle + \"histories_siamese_final.pickle\", \"wb\"))\n",
    "        \n",
    "#pickle.dump(history.history, open(hp.path_pickle + \"history.pickle\", \"wb\"))\n",
    "#pickle.dump(val_callback.val_accuracy, open(hp.path_pickle + \"val_acc.pickle\", \"wb\"))\n",
    "#pickle.dump(val_callback.confusion_matrices_perc, open(hp.path_pickle + \"conf.pickle\", \"wb\"))"
   ],
   "metadata": {
    "id": "IMkBlwP7PTaS",
    "colab_type": "code",
    "colab": {},
    "scrolled": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 : Train single model"
   ],
   "metadata": {
    "id": "N6sMXLZhP0hI",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SINGLE NET\n",
    "\n",
    "model = covid_net_single(nf=hp.nf, fc_units=hp.fc_units, single=True)\n",
    "adam = keras.optimizers.Adam(learning_rate=hp.learning_rate)\n",
    "model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "save_callback = keras.callbacks.ModelCheckpoint(\"Models/singleNet_resized\", monitor='val_acc', save_best_only=True, save_weights_only=False)\n",
    "#tb_callback = keras.callbacks.TensorBoard(log_dir=\"Logs\")\n",
    "\n",
    "training_generator = DataGeneratorSingle(X_normal_tr, X_pneumonia_tr, X_covid_tr, num_channels=1,\n",
    "                                 batch_size=hp.batch_size_tr)\n",
    "\n",
    "history = model.fit_generator(generator=training_generator, validation_data=(X_val_single, Y_val_single),\n",
    "                                  epochs=hp.n_epochs, callbacks=[save_callback], use_multiprocessing=False, workers=hp.workers)\n",
    "\n",
    "\n",
    "#pickle.dump(history.history, open(hp.path_pickle + \"history.pickle\", \"wb\"))\n",
    "#pickle.dump(val_callback.val_accuracy, open(hp.path_pickle + \"val_acc.pickle\", \"wb\"))\n",
    "#pickle.dump(val_callback.confusion_matrices_perc, open(hp.path_pickle + \"conf.pickle\", \"wb\"))"
   ],
   "metadata": {
    "id": "3AiPfzCmDAEU",
    "colab_type": "code",
    "outputId": "fc062740-61f0-4721-bec2-f7f31704fc00",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "scrolled": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 : Confusion matrix"
   ],
   "metadata": {
    "id": "BSABG-u1RA4e",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#history = pickle.load(open(hp.path_pickle + \"history.pickle\", \"rb\"))\n",
    "#val_acc = pickle.load(open(hp.path_pickle + \"val_acc.pickle\", \"rb\"))\n",
    "val_acc_3w = val_callback.val_accuracy\n",
    "\n",
    "best = int(np.argmax(val_acc_3w))\n",
    "best_val_acc = val_acc_3w[best]\n",
    "best_conf = val_callback.confusion_matrices_perc[best]\n",
    "\n",
    "print(\"Highest val acc:\", best_val_acc)\n",
    "\n",
    "df_cm = pd.DataFrame(best_conf, index=[\"covid\", \"normal\", \"pneumonia\"],  columns=[\"covid\", \"normal\", \"pneumonia\"])\n",
    "seaborn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.savefig(hp.path_figs + \"fig3_conf.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 : Plotting"
   ],
   "metadata": {
    "id": "o2c6EHy5RFEo",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "histories = pickle.load(open(\"histories_siamese_final.pickle\", \"rb\"))\n",
    "histories_single = pickle.load(open(\"histories_128_single.pickle\", \"rb\"))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "0.736\n28\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Siamese\n",
    "histories = pickle.load(open(hp.path_pickle + \"history.pickle\", \"rb\"))\n",
    "\n",
    "epochs = np.arange(1, hp.n_epochs + 1)\n",
    "loss = histories[\"loss\"]\n",
    "acc = histories[\"acc\"]\n",
    "val_acc_3w = histories[\"val_acc\"]\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, loss, label=\"training loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both', \n",
    "    bottom=False, \n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, label=\"training accuracy\")\n",
    "plt.plot(epochs, val_acc_3w, label=\"3-way validation accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend(fontsize=\"x-small\", loc=(0.56, 0.55))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(hp.path_figs + \"siameseFinalRun.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "u-ONxBG27bU9",
    "colab_type": "code",
    "outputId": "12ac184d-731f-4f14-fef6-928ebe0fcbb0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Single net\n",
    "histories = pickle.load(open(hp.path_pickle + \"history.pickle\", \"rb\"))\n",
    "\n",
    "epochs = np.arange(1,  101)\n",
    "loss = histories[\"loss\"]\n",
    "acc = histories[\"acc\"]\n",
    "val_loss = histories[\"val_loss\"]\n",
    "val_acc = histories[\"val_acc\"]\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(epochs, loss, label=\"training loss\")\n",
    "plt.plot(epochs, val_loss, label=\"validation loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both', \n",
    "    bottom=False, \n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "\n",
    "plt.legend(fontsize=\"x-small\", loc=\"upper left\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, label=\"training accuracy\")\n",
    "plt.plot(epochs, val_acc, label=\"validation accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.yticks([0.6, 0.7, 0.8, 0.9])\n",
    "plt.legend(fontsize=\"x-small\", loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(hp.path_figs + \"singleFinalRun_cropped.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.0 Testing"
   ],
   "metadata": {
    "id": "KYTIG0z7UNvY",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Training data is used to create class templates for testing the siamese network\n",
    "X_normal_tr = pickle.load(open(hp.path_pickle + \"X_normal_tr.pickle\", \"rb\"))\n",
    "X_pneumonia_tr = pickle.load(open(hp.path_pickle + \"X_pneumonia_tr.pickle\", \"rb\"))\n",
    "X_covid_tr = pickle.load(open(hp.path_pickle + \"X_covid_tr.pickle\", \"rb\"))\n",
    "\n",
    "# Test data\n",
    "X_normal_ts = pickle.load(open(hp.path_pickle + \"X_normal_ts.pickle\", \"rb\"))\n",
    "X_pneumonia_ts = pickle.load(open(hp.path_pickle + \"X_pneumonia_ts.pickle\", \"rb\"))\n",
    "X_covid_ts = pickle.load(open(hp.path_pickle + \"X_covid_ts.pickle\", \"rb\"))\n",
    "\n",
    "N_covid_tr = X_covid_tr.shape[0]\n",
    "N_normal_tr = X_normal_tr.shape[0]\n",
    "N_pneumonia_tr = X_pneumonia_tr.shape[0]\n",
    "\n",
    "N_covid_val = X_covid_val.shape[0]\n",
    "N_normal_val = X_normal_val.shape[0]\n",
    "N_pneumonia_val = X_pneumonia_val.shape[0]\n",
    "\n",
    "N_covid_ts = X_covid_ts.shape[0]\n",
    "N_normal_ts = X_normal_ts.shape[0]\n",
    "N_pneumonia_ts = X_pneumonia_ts.shape[0]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Load models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Pretrained models, both single covid-net and a siamese net\n",
    "model_siamese = keras.models.load_model(\"Models/siameseNet\")\n",
    "model_single_resized = keras.models.load_model(\"Models/singleNet_resized\")\n",
    "model_single = keras.models.load_model(\"Models/singleNet\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 Test single model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Concatenate test data\n",
    "X_ts = np.concatenate((X_covid_ts_R, X_normal_ts_R, X_pneumonia_ts_R)).reshape(-1, 224, 224, 1)\n",
    "\n",
    "Y_ts = [0 for i in range(N_covid_ts)]\n",
    "Y_ts.extend([1 for i in range(N_normal_ts)])\n",
    "Y_ts.extend([2 for i in range(N_pneumonia_ts)])\n",
    "\n",
    "Y_ts = np.asarray(Y_ts)\n",
    "\n",
    "predictions = model_single_resized.predict(X_ts)\n",
    "Y_predicted = []\n",
    "\n",
    "for pred in predictions:\n",
    "        Y_predicted.append(np.argmax(pred))\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 139,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion matrix total numbers, single net\n",
    "conf = confusion_matrix(Y_ts, Y_predicted) \n",
    "\n",
    "# Total accuracy\n",
    "print((conf[0, 0] + conf[1, 1] + conf[2, 2]) / np.sum(conf))\n",
    "\n",
    "seaborn.set(font_scale=1.1) \n",
    "df_cm = pd.DataFrame(conf, index=[\"C\", \"N\", \"P\"],  columns=[\"C\", \"N\", \"P\"])\n",
    "ax = seaborn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt=\"d\")\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(hp.path_figs + \"confSingleNet_numbers_cropped.pdf\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion matrix percentage for single net. Total percentage 81% \n",
    "conf = np.round(conf / np.sum(conf, axis=1).reshape(-1, 1), 2)\n",
    "\n",
    "seaborn.set(font_scale=1.1) \n",
    "df_cm = pd.DataFrame(conf, index=[\"C\", \"N\", \"P\"],  columns=[\"C\", \"N\", \"P\"])\n",
    "ax = seaborn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "plt.savefig(hp.path_figs + \"confSingleNet_78p_cropped.pdf\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 Generate class templates for siamese model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a model that returns the feature vector calculated by the covid-net\n",
    "input1 = model_siamese.layers[2].get_input_at(0)\n",
    "output1 = model_siamese.layers[2].get_output_at(0)\n",
    "model_feat_vec = Model(inputs=input1, outputs=output1)\n",
    "\n",
    "# Create tempalates as the average over all feature vectors calculated from the training data\n",
    "covid_template = np.zeros((1, 256))\n",
    "normal_template = np.zeros((1, 256))\n",
    "pneumonia_template = np.zeros((1, 256))\n",
    "\n",
    "for i in range(N_covid_tr):\n",
    "    covid_img = X_covid_tr[i].reshape(224, 224, 1)\n",
    "    covid_vec = model_feat_vec.predict([[covid_img]])\n",
    "    covid_template += covid_vec\n",
    "\n",
    "for i in range(N_normal_tr):\n",
    "    normal_img = X_normal_tr[i].reshape(224, 224, 1)\n",
    "    normal_vec = model_feat_vec.predict([[normal_img]])\n",
    "    normal_template += normal_vec\n",
    "\n",
    "for i in range(N_pneumonia_tr):\n",
    "    pneumonia_img = X_pneumonia_tr[i].reshape(224, 224, 1)\n",
    "    pneumonia_vec = model_feat_vec.predict([[pneumonia_img]])\n",
    "    pneumonia_template += pneumonia_vec\n",
    "\n",
    "covid_template /= N_covid_tr\n",
    "normal_template /= N_normal_tr\n",
    "pneumonia_template /= N_pneumonia_tr\n",
    "\n",
    "#pickle.dump(covid_template, open(hp.path_pickle + \"covid_template.pickle\", \"wb\"))\n",
    "#pickle.dump(normal_template, open(hp.path_pickle + \"normal_template.pickle\", \"wb\"))\n",
    "#pickle.dump(pneumonia_template, open(hp.path_pickle + \"pneumonia_template.pickle\", \"wb\"))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.6 Test siamese network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def test_siamese(model, X_covid_ts, X_normal_ts, X_pneumonia_ts, covid_template, normal_template, pneumonia_template):\n",
    "    \"\"\"Functions to test the accuracy of the siamese network on the test data. For each test image, a similarity score\n",
    "    is calculated between the image and all three class templates. The image is predicted to have the same class as\n",
    "    the class that produced the smallest similarity measure (since same class images should have output 0 and different classes\n",
    "    should have output 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    N_covid = X_covid_ts.shape[0]\n",
    "    N_normal = X_normal_ts.shape[0]\n",
    "    N_pneumonia = X_pneumonia_ts.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    correct = []\n",
    "    \n",
    "    for i in range(N_covid):\n",
    "        img_covid = X_covid_ts[i].reshape(224, 224, 1)\n",
    "        \n",
    "        covid_sim = model.predict([[img_covid], covid_template])\n",
    "        normal_sim = model.predict([[img_covid], normal_template])\n",
    "        pneumonia_sim = model.predict([[img_covid], pneumonia_template])\n",
    "        \n",
    "        sim = [covid_sim, normal_sim, pneumonia_sim]\n",
    "        \n",
    "        correct.append(0)\n",
    "        predictions.append(np.argmin(sim))\n",
    "    \n",
    "    for i in range(N_normal):\n",
    "        img_normal = X_normal_ts[i].reshape(224, 224, 1)\n",
    "        \n",
    "        covid_sim = model.predict([[img_normal], covid_template])\n",
    "        normal_sim = model.predict([[img_normal], normal_template])\n",
    "        pneumonia_sim = model.predict([[img_normal], pneumonia_template])\n",
    "        \n",
    "        sim = [covid_sim, normal_sim, pneumonia_sim]\n",
    "        \n",
    "        correct.append(1)\n",
    "        predictions.append(np.argmin(sim))\n",
    "    \n",
    "    for i in range(N_pneumonia):\n",
    "        img_pneumonia = X_pneumonia_ts[i].reshape(224, 224, 1)\n",
    "        \n",
    "        covid_sim = model.predict([[img_pneumonia], covid_template])\n",
    "        normal_sim = model.predict([[img_pneumonia], normal_template])\n",
    "        pneumonia_sim = model.predict([[img_pneumonia], pneumonia_template])\n",
    "        \n",
    "        sim = [covid_sim, normal_sim, pneumonia_sim]\n",
    "        \n",
    "        correct.append(2)\n",
    "        predictions.append(np.argmin(sim))\n",
    "    \n",
    "    return np.asarray(correct), np.asarray(predictions)\n",
    "\n",
    "def siamese_feature_extraction(model_siamese):\n",
    "    \"\"\"Create a new model based on the siamese network that takes an image as input and\n",
    "    returns the calculated feature vector.\n",
    "    \"\"\"\n",
    "    input_siamese = model_siamese.layers[2].get_input_at(0)\n",
    "    output_siamese = model_siamese.layers[2].get_output_at(0)\n",
    "    \n",
    "    model = Model(inputs=input_siamese, outputs=output_siamese)\n",
    "    return model\n",
    "\n",
    "def model_final(model_siamese):\n",
    "    \"\"\"Final model used for testing.\"\"\"\n",
    "    \n",
    "    input1 = Input(shape=(224, 224, 1))    # Input test image\n",
    "    \n",
    "    feature_extraction = siamese_feature_extraction(model_siamese)    # Feature extraction model\n",
    "    \n",
    "    feature_vec1 = feature_extraction(input1)\n",
    "    feature_vec2 = Input(shape=256)    # Class template feature vector input\n",
    "    \n",
    "    l1 = Lambda(lambda tensors : keras.backend.abs(tensors[0] - tensors[1]), name=\"Lambda\")([feature_vec1, feature_vec2])\n",
    "\n",
    "    similarity = model_siamese.get_layer(\"Similarity\")(l1)\n",
    "\n",
    "    model_final = Model(inputs=[input1, feature_vec2], outputs=similarity)\n",
    "    \n",
    "    return model_final\n",
    "\n",
    "covid_template = pickle.load(open(hp.path_pickle + \"covid_template.pickle\", \"rb\"))\n",
    "normal_template = pickle.load(open(hp.path_pickle + \"normal_template.pickle\", \"rb\"))\n",
    "pneumonia_template = pickle.load(open(hp.path_pickle + \"pneumonia_template.pickle\", \"rb\"))\n",
    "\n",
    "model = model_final(model_siamese)\n",
    "\n",
    "correct, prediction = test_siamese(model, X_covid_ts, X_normal_ts, X_pneumonia_ts, covid_template, normal_template, pneumonia_template)\n",
    "\n",
    "bool_array = (correct == prediction)    # T/F array indication if the image was classified correctly\n",
    "num_correct = np.count_nonzero(bool_array)\n",
    "print(num_correct / len(correct))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion matrix total numbers for siamese net\n",
    "conf = confusion_matrix(correct, prediction)\n",
    "\n",
    "seaborn.set(font_scale=1.1)\n",
    "df_cm = pd.DataFrame(conf, index=[\"C\", \"N\", \"P\"],  columns=[\"C\", \"N\", \"P\"])\n",
    "ax = seaborn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d')\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "#plt.savefig(hp.path_figs + \"confSiameseNet_numbers.pdf\")\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Confusion matrix percentage for siamese model. Total accuracy 87%\n",
    "conf = np.round(conf / np.sum(conf, axis=1).reshape(-1, 1), 2)\n",
    "\n",
    "seaborn.set(font_scale=1.1)\n",
    "df_cm = pd.DataFrame(conf, index=[\"C\", \"N\", \"P\"],  columns=[\"C\", \"N\", \"P\"])\n",
    "ax = seaborn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "ax.set_xlabel(\"Predicted label\", fontsize=14)\n",
    "ax.set_ylabel(\"True label\", fontsize=14)\n",
    "plt.yticks(rotation=0)\n",
    "#plt.savefig(hp.path_figs + \"confSiameseNet_87p.pdf\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.0 : Extra"
   ],
   "metadata": {
    "id": "SXCzV6dIVF_i",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 : Image pre-processing and pickle"
   ],
   "metadata": {
    "id": "ABf4M_TAP1Tc",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Read in images, resize to 224x224, convert to gray scale, store numpy arrays using pickle\n",
    "\n",
    "path_train = \"/content/drive/My Drive/Covid-19_datasets/data/train/\"\n",
    "path_test = \"/content/drive/My Drive/Covid-19_datasets/data/test/\"\n",
    "path_pickle = \"/content/drive/My Drive/Covid-19_datasets/data/Pickles/\"\n",
    "train_dict = pickle.load(open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/train_dict.pickle\", \"rb\"))\n",
    "test_dict = pickle.load(open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/test_dict.pickle\", \"rb\"))\n",
    "\n",
    "X_covid_tr = []\n",
    "X_pneumonia_tr = []\n",
    "X_normal_tr = []\n",
    "X_covid_ts = []\n",
    "X_pneumonia_ts = []\n",
    "X_normal_ts = []\n",
    "\n",
    "# Input dimensions used in COVID-Net\n",
    "w = 224 \n",
    "h = 224\n",
    "\n",
    "for filename in os.listdir(path_train):\n",
    "    if filename == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    img = cv2.imread(path_train + filename, cv2.IMREAD_GRAYSCALE)   # Convert to grayscale??\n",
    "    img = cv2.resize(img, (h, w), interpolation=cv2.INTER_AREA)\n",
    "    # plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    if train_dict[filename] == \"normal\":\n",
    "        X_normal_tr.append(img)\n",
    "    elif train_dict[filename] == \"pneumonia\":\n",
    "        X_pneumonia_tr.append(img)\n",
    "    elif train_dict[filename] == \"COVID-19\":\n",
    "        X_covid_tr.append(img)\n",
    "    else:\n",
    "        raise ValueError(\"Something wrong with dictionary\")\n",
    "    break   # Remove this to read in whole data set\n",
    "\n",
    "print(\"Train finished\")\n",
    "\n",
    "for filename in os.listdir(path_test):\n",
    "    if filename == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    img = cv2.imread(path_test + filename, cv2.IMREAD_GRAYSCALE)   # Convert to grayscale?\n",
    "    img = cv2.resize(img, (h, w), interpolation=cv2.INTER_AREA)\n",
    "    # plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    if test_dict[filename] == \"normal\":\n",
    "        X_normal_ts.append(img)\n",
    "    elif test_dict[filename] == \"pneumonia\":\n",
    "        X_pneumonia_ts.append(img)\n",
    "    elif test_dict[filename] == \"COVID-19\":\n",
    "        X_covid_ts.append(img)\n",
    "    else:\n",
    "        raise ValueError(\"Something wrong with dictionary\")\n",
    "    break   # Remove this to read in whole data set\n",
    "\n",
    "X_covid_tr = np.asarray(X_covid_tr)\n",
    "X_pneumonia_tr = np.asarray(X_pneumonia_tr)\n",
    "X_normal_tr = np.asarray(X_normal_tr)\n",
    "X_covid_ts = np.asarray(X_covid_ts)\n",
    "X_pneumonia_ts = np.asarray(X_pneumonia_ts)\n",
    "X_normal_ts = np.asarray(X_normal_ts)\n",
    "\n",
    "#pickle.dump(X_covid_tr, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_covid_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_normal_tr, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_normal_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_tr, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_pneumonia_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_covid_ts, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_covid_ts.pickle\", \"wb\"))\n",
    "#pickle.dump(X_normal_ts, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_normal_ts.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_ts, open(\"/content/drive/My Drive/Covid-19_datasets/data/Pickles/X_pneumonia_ts.pickle\", \"wb\"))"
   ],
   "metadata": {
    "id": "uiYf7tDY1a-f",
    "colab_type": "code",
    "colab": {}
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocess the unprocessed arrays stored in Pickles by normalizing the pixel\n",
    "# values and then normalizing the data w.r.t. the training mean and std.\n",
    "# The preprocessed data is stored in Pickles2\n",
    "\n",
    "path_pickle = \"\"\n",
    "\n",
    "# Training data\n",
    "X_normal_tr = pickle.load(open(path_pickle + \"X_normal_tr.pickle\", \"rb\")) / 255.0\n",
    "X_pneumonia_tr = pickle.load(open(path_pickle + \"X_pneumonia_tr.pickle\", \"rb\")) / 255.0\n",
    "X_covid_tr = pickle.load(open(path_pickle + \"X_covid_tr.pickle\", \"rb\")) / 255.0\n",
    "# Testing data\n",
    "X_normal_val = pickle.load(open(path_pickle + \"X_normal_val.pickle\", \"rb\")) / 255.0\n",
    "X_pneumonia_val = pickle.load(open(path_pickle + \"X_pneumonia_val.pickle\", \"rb\")) / 255.0\n",
    "X_covid_val = pickle.load(open(path_pickle + \"X_covid_val.pickle\", \"rb\")) / 255.0\n",
    "# Testing data\n",
    "X_normal_ts = pickle.load(open(path_pickle + \"X_normal_ts.pickle\", \"rb\")) / 255.0\n",
    "X_pneumonia_ts = pickle.load(open(path_pickle + \"X_pneumonia_ts.pickle\", \"rb\")) / 255.0\n",
    "X_covid_ts = pickle.load(open(path_pickle + \"X_covid_ts.pickle\", \"rb\")) / 255.0\n",
    "\n",
    "# Calculate mean and std of training data\n",
    "X_tr = np.concatenate((X_normal_tr, X_pneumonia_tr, X_covid_tr), axis=0)\n",
    "X_mean = np.mean(X_tr, axis=0)\n",
    "X_std = np.std(X_tr, axis=0, ddof=1)\n",
    "\n",
    "# Normalize all data w.r.t. training mean and std\n",
    "X_normal_tr = (X_normal_tr - X_mean) / X_std\n",
    "X_pneumonia_tr = (X_pneumonia_tr - X_mean) / X_std\n",
    "X_covid_tr = (X_covid_tr - X_mean) / X_std\n",
    "\n",
    "X_normal_val = (X_normal_val - X_mean) / X_std\n",
    "X_pneumonia_val = (X_pneumonia_val - X_mean) / X_std\n",
    "X_covid_val = (X_covid_val - X_mean) / X_std\n",
    "\n",
    "X_normal_ts = (X_normal_ts - X_mean) / X_std\n",
    "X_pneumonia_ts = (X_pneumonia_ts - X_mean) / X_std\n",
    "X_covid_ts = (X_covid_ts - X_mean) / X_std\n",
    "\n",
    "path_pickle = \"\"\n",
    "\n",
    "pickle.dump(X_mean, open(path_pickle + \"X_mean.pickle\", \"wb\"))\n",
    "pickle.dump(X_std, open(path_pickle + \"X_std.pickle\", \"wb\"))\n",
    "\n",
    "#pickle.dump(X_normal_tr, open(path_pickle + \"X_normal_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_tr, open(path_pickle + \"X_pneumonia_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_covid_tr, open(path_pickle + \"X_covid_tr.pickle\", \"wb\"))\n",
    "\n",
    "#pickle.dump(X_normal_val, open(path_pickle + \"X_normal_val.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_val, open(path_pickle + \"X_pneumonia_val.pickle\", \"wb\"))\n",
    "#pickle.dump(X_covid_val, open(path_pickle + \"X_covid_val.pickle\", \"wb\"))\n",
    "\n",
    "#pickle.dump(X_normal_ts, open(path_pickle + \"X_normal_ts.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_ts, open(path_pickle + \"X_pneumonia_ts.pickle\", \"wb\"))\n",
    "#pickle.dump(X_covid_ts, open(path_pickle + \"X_covid_ts.pickle\", \"wb\"))"
   ],
   "metadata": {
    "id": "bO5ey3ppsFve",
    "colab_type": "code",
    "outputId": "4d045c58-fbc3-4e29-c4c5-d3a8b8de254e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 : Validation set creation and pickle"
   ],
   "metadata": {
    "id": "EuH2SjRYP7nb",
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create validation data\n",
    "\n",
    "# Shuffle data\n",
    "np.random.shuffle(X_covid_tr_full)\n",
    "np.random.shuffle(X_covid_ts)\n",
    "np.random.shuffle(X_normal_tr_full)\n",
    "np.random.shuffle(X_normal_ts)\n",
    "np.random.shuffle(X_pneumonia_tr_full)\n",
    "np.random.shuffle(X_pneumonia_ts)\n",
    "\n",
    "perc_val = 0.2\n",
    "N_covid_val = int(perc_val * N_covid_tr_full)\n",
    "N_normal_val = int(perc_val * N_normal_tr_full)\n",
    "N_pneumonia_val = int(perc_val * N_pneumonia_tr_full)\n",
    "\n",
    "# Covid images\n",
    "mask_val = np.ones(N_covid_tr, dtype=bool)\n",
    "mask_val[N_covid_val:] = False\n",
    "mask_tr = [not mask for mask in mask_val]\n",
    "X_covid_val = X_covid_tr_full[mask_val, :, :]\n",
    "X_covid_tr = X_covid_tr_full[mask_tr, :, :]\n",
    "\n",
    "# Normal images\n",
    "mask_val = np.ones(N_normal_tr, dtype=bool)\n",
    "mask_val[N_normal_val:] = False\n",
    "mask_tr = [not mask for mask in mask_val]\n",
    "X_normal_val = X_normal_tr_full[mask_val, :, :]\n",
    "X_normal_tr = X_normal_tr_full[mask_tr, :, :]\n",
    "\n",
    "# Pneumonia images\n",
    "mask_val = np.ones(N_pneumonia_tr, dtype=bool)\n",
    "mask_val[N_pneumonia_val:] = False\n",
    "mask_tr = [not mask for mask in mask_val]\n",
    "X_pneumonia_val = X_pneumonia_tr_full[mask_val, :, :]\n",
    "X_pneumonia_tr = X_pneumonia_tr_full[mask_tr, :, :]\n",
    "\n",
    "\n",
    "#path_pickle = \"/content/drive/My Drive/Covid-19_datasets/data/Pickles/\"\n",
    "#pickle.dump(X_covid_tr, open(path_pickle + \"X_covid_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_covid_val, open(path_pickle + \"X_covid_val.pickle\", \"wb\"))\n",
    "#pickle.dump(X_normal_tr, open(path_pickle + \"X_normal_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_normal_val, open(path_pickle + \"X_normal_val.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_tr, open(path_pickle + \"X_pneumonia_tr.pickle\", \"wb\"))\n",
    "#pickle.dump(X_pneumonia_val, open(path_pickle + \"X_pneumonia_val.pickle\", \"wb\"))\n"
   ],
   "metadata": {
    "id": "WSgsi8SZrsrj",
    "colab_type": "code",
    "colab": {}
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "N_covid_val = len(X_covid_val)\n",
    "N_normal_val = len(X_normal_val)\n",
    "N_pneumonia_val = len(X_pneumonia_val)\n",
    "\n",
    "index_normal = np.random.choice(N_normal_val, N_covid_val, replace=False)\n",
    "index_pneumonia = np.random.choice(N_pneumonia_val, N_covid_val, replace=False)\n",
    "\n",
    "X_covid_val = X_covid_val.reshape(-1, 224, 224, 1)\n",
    "X_normal_val = X_normal_val[index_normal, :, :].reshape(-1, 224, 224, 1)\n",
    "X_pneumonia_val = X_pneumonia_val[index_pneumonia, :, :].reshape(-1, 224, 224, 1)\n",
    "\n",
    "Y_val = [0 for i in range(N_covid_val)]\n",
    "Y_val.extend([1 for i in range(N_covid_val)])\n",
    "Y_val.extend([2 for i in range(N_covid_val)])\n",
    "\n",
    "X_val = np.concatenate((X_covid_val, X_normal_val, X_pneumonia_val))\n",
    "\n",
    "p = np.random.permutation(len(Y_val))\n",
    "\n",
    "Y_val = np.asarray(Y_val)\n",
    "\n",
    "X_val = X_val[p]\n",
    "Y_val = Y_val[p]\n",
    "\n",
    "path_pickle = \"Resized/\"\n",
    "pickle.dump(X_val, open(path_pickle + \"X_val_single.pickle\", \"wb\"))\n",
    "pickle.dump(Y_val, open(path_pickle + \"Y_val_single.pickle\", \"wb\"))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load validation sets\n",
    "X_covid_val_R = pickle.load(open(\"Resized/X_covid_val.pickle\", \"rb\"))\n",
    "X_pneumonia_val_R = pickle.load(open(\"Resized/X_pneumonia_val.pickle\", \"rb\"))\n",
    "X_normal_val_R = pickle.load(open(\"Resized/X_normal_val.pickle\", \"rb\"))\n",
    "\n",
    "X_covid_val = pickle.load(open(\"X_covid_val.pickle\", \"rb\"))\n",
    "X_pneumonia_val = pickle.load(open(\"X_pneumonia_val.pickle\", \"rb\"))\n",
    "X_normal_val = pickle.load(open(\"X_normal_val.pickle\", \"rb\"))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load test sets\n",
    "X_covid_ts_R = pickle.load(open(\"Resized/X_covid_ts.pickle\", \"rb\"))\n",
    "X_pneumonia_ts_R = pickle.load(open(\"Resized/X_pneumonia_ts.pickle\", \"rb\"))\n",
    "X_normal_ts_R = pickle.load(open(\"Resized/X_normal_ts.pickle\", \"rb\"))\n",
    "\n",
    "X_covid_ts = pickle.load(open(\"X_covid_ts.pickle\", \"rb\"))\n",
    "X_pneumonia_ts = pickle.load(open(\"X_pneumonia_ts.pickle\", \"rb\"))\n",
    "X_normal_ts = pickle.load(open(\"X_normal_ts.pickle\", \"rb\"))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# GradCAM, as implemented in https://github.com/eclique/keras-gradcam\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "\n",
    "H, W = 224, 224 # Input shape, defined by the model (model.input_shape)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def load_image(path, x_mean, x_std):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "    x = x.astype(dtype=np.float64)\n",
    "    x /= 255\n",
    "    x = (x - x_mean) / x_std\n",
    "    \n",
    "    return np.asarray(x)\n",
    "   \n",
    "def load_rgbimage(path, preprocess=True):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    return np.asarray(x)\n",
    "   \n",
    "def normalize(x):\n",
    "    \"\"\"L2 norm function\"\"\"\n",
    "    return (x + 1e-10) / (K.sqrt(K.mean(K.square(x))) + 1e-10)\n",
    "\n",
    "def grad_cam(input_model, image, cls, layer_name):\n",
    "    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n",
    "    #Target class output\n",
    "    y_c = input_model.get_layer(layer_name).output\n",
    "    \n",
    "    #Map of activations\n",
    "    conv_output = input_model.get_layer(layer_name).output\n",
    "    #Get the gradient of the socre of the class (y_c) with respect of the map of activations.\n",
    "    grads = K.gradients(y_c, conv_output)[0]\n",
    "    # Normalize \n",
    "    grads = normalize(grads)\n",
    "    gradient_function = K.function([input_model.input], [conv_output, grads])\n",
    "    #Mean of all the activation maps along the channels and the result obtained is the final class discriminative saliency map (weights).\n",
    "    output, grads_val = gradient_function([image])  \n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "    weights = np.mean(grads_val, axis=(0, 1)) \n",
    "    cam = np.dot(output, weights)\n",
    "    # Process CAM, ReLU applied\n",
    "    cam = cv2.resize(cam, (W, H), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0) #ReLU\n",
    "    cam_max = cam.max() \n",
    "    if cam_max != 0: \n",
    "        cam = cam / cam_max\n",
    "    return cam\n",
    "\n",
    "\n",
    "def compute_saliency(model, X_mean, X_std, sets, layer_name='block5_conv3', original=False, cls=-1, visualize=True):\n",
    "    \"\"\"Compute saliency using all three approaches.\n",
    "        -layer_name: layer to compute gradients;\n",
    "        -cls: class number to localize (-1 for most probable class).\n",
    "    \"\"\"\n",
    "    preprocessed_input = sets[4]   #Input image is ovewritten here, can be changed by any image in the set.\n",
    "    preim = preprocessed_input.reshape(-1,224,224,1)\n",
    "    predictions = model.predict(x=preim)\n",
    "    if cls == -1:\n",
    "        cls = np.argmax(predictions)    \n",
    "    gradcam = grad_cam(model, preim, cls, layer_name)\n",
    "    \n",
    "    if visualize:\n",
    "        preprocessed_input = np.multiply(preprocessed_input, X_std) + X_mean\n",
    "        preprocessed_input *= 255\n",
    "        preprocessed_input = preprocessed_input.astype(dtype=np.int)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.subplot(121)\n",
    "        plt.title('GradCAM - Normal')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(preprocessed_input, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        plt.imshow(gradcam, cmap='jet', alpha=0.5)\n",
    "        \n",
    "        if original:\n",
    "            plt.subplot(122)\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(preprocessed_input, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        \n",
    "        plt.savefig('Figures/normal_bad_4.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "    return gradcam\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Mean and STD\n",
    "X_mean = pickle.load(open(\"X_mean.pickle\", \"rb\"))\n",
    "X_std = pickle.load(open(\"X_std.pickle\", \"rb\"))\n",
    "#Compute GradCAM\n",
    "gradcam1 = compute_saliency(model_single, layer_name='Output_before_SM', cls=-1, visualize=True, X_mean=X_mean, X_std=X_std, sets=X_normal_ts, original=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}
